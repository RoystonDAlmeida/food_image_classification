{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNT8QjNvSxNZ"
      },
      "outputs": [],
      "source": [
        "# Install the dependencies\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install tensorflow-hub\n",
        "!pip install pillow\n",
        "!pip install -U -q tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "t4SS2rZ7T3h4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the Food-101 Dataset\n",
        "# Store the unprocessed raw data\n",
        "(train_data_raw, test_data_raw), info = tfds.load(\n",
        "    'food101',\n",
        "    split=['train', 'validation'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "metadata": {
        "id": "_6DHsSIqVDqz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing and augmentation\n",
        "image_size = (128, 128)\n",
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Calculate the total number of training samples\n",
        "total_train_samples = len(train_data_raw)\n",
        "steps_per_epoch = total_train_samples // BATCH_SIZE\n",
        "\n",
        "def preprocess(image, label):\n",
        "  \"\"\"Preprocesses an image and label for the model.\"\"\"\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)  # Converts image data type to float32\n",
        "  # Check if image has 3 channels (RGB)\n",
        "  if image.shape[-1] != 3:\n",
        "      image = tf.expand_dims(image, axis=-1)  # Add a channel dimension if it's missing (for grayscale images)\n",
        "      image = tf.image.grayscale_to_rgb(image)  # Convert grayscale to RGB using tf.image.grayscale_to_rgb\n",
        "  image = tf.image.resize(image, image_size)  # Convert image to consistent size\n",
        "  label = tf.one_hot(label, num_classes)  # Converts class labels to vectors, to use categorical_crossentropy loss\n",
        "  return image, label\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),  # Rotate the image horizontally\n",
        "        layers.RandomRotation(0.1),       # Randomly rotates images by a small angle, done to improve generalization\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the data generator function\n",
        "def data_generator(dataset):\n",
        "    \"\"\"Yields preprocessed images and labels from the dataset. Yield ensures data loaded on demand\"\"\"\n",
        "    for image, label in dataset:\n",
        "        yield preprocess(image, label)\n",
        "\n",
        "# Create the tf.data.Dataset for training data using the generator\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_data_raw),  # Using train_data_raw here\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((128, 128, 3), (num_classes,))\n",
        ").batch(BATCH_SIZE).prefetch(BUFFER_SIZE)\n",
        "\n",
        "# These lines preprocess the data for the test/validation set\n",
        "# (consider using a generator here as well if you're still facing memory constraints)\n",
        "test_data = test_data_raw.map(preprocess).batch(BATCH_SIZE).prefetch(BUFFER_SIZE)\n",
        "\n",
        "# Enable mixed precision training\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "rgyDnEg8VNf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: # Build the model (using transfer learning with EfficientNetB0)\n",
        "def build_model():\n",
        "  base_model = tf.keras.applications.EfficientNetB0(\n",
        "      include_top=False, weights='imagenet', input_shape=(128, 128, 3)\n",
        "  )\n",
        "\n",
        "  base_model.trainable = False # Freeze weights of base model initially to prevent large changes during training\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      base_model,\n",
        "      data_augmentation,\n",
        "      layers.GlobalAveragePooling2D(),\n",
        "      layers.Dropout(0.2),  # Randomly to drop out neurons during training to prevent overfitting\n",
        "      layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model()"
      ],
      "metadata": {
        "id": "6gtRD2MXVQcP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compile and Train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model with garbage collection\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    history = model.fit(train_data, epochs=1, steps_per_epoch=steps_per_epoch, validation_data=test_data)\n",
        "    gc.collect()  # Force garbage collection after each epoch"
      ],
      "metadata": {
        "id": "KfwfBOofVWqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "UNTIW9MSVa4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Training History\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L-WToq1WVicV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}